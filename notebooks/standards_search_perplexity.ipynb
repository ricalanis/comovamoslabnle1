{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b556b86a-fe24-4f64-8d42-d0e22eb76ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_webpage_text(url, timeout=30):\n",
    "    \"\"\"\n",
    "    Fetches a webpage and extracts its text content, removing scripts, styles, and other non-content elements.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the webpage to fetch\n",
    "        timeout (int): Request timeout in seconds\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text content of the webpage\n",
    "    \n",
    "    Raises:\n",
    "        requests.RequestException: If there's an error fetching the webpage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set a custom User-Agent to avoid potential blocks\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url, headers=headers, timeout=timeout, verify=False)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup(['script', 'style', 'head', 'title', 'meta', '[document]']):\n",
    "            element.decompose()\n",
    "            \n",
    "        # Get text and clean it\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean the extracted text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        # Remove extra whitespace and normalize spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Error fetching webpage: {str(e)}\")\n",
    "\n",
    "def get_webpage_text(url):\n",
    "    \"\"\"\n",
    "    Simplified wrapper function for quick text extraction.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the webpage\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return extract_webpage_text(url)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "382de7ed-aeaa-4483-b67a-f76a6c6fe000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'catalogodatos.nl.gob.mx'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efd5c9cb-a82f-43b7-8a02-ee8098192434",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = json.load(open(\"data_quality_report.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "048c0318-29c3-4928-8bc6-2aab62a7fd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'standard': 'Office of Personnel Management (OPM) Data Standards',\n",
       "  'match_grade': 'green',\n",
       "  'dataset_link': 'https://dw.opm.gov/datastandards/overview'},\n",
       " {'standard': 'National Information Exchange Model (NIEM)',\n",
       "  'match_grade': 'yellow',\n",
       "  'dataset_link': 'https://www.epa.gov/data/federal-national-and-international-data-standards'},\n",
       " {'standard': 'Digital Accountability and Transparency Act (DATA Act) Information Model Schema (DAIMS)',\n",
       "  'match_grade': 'red',\n",
       "  'dataset_link': 'https://fiscal.treasury.gov/data-transparency/history-overview.html'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "YOUR_API_KEY = \"{API KEY}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476ae51-eac4-4338-b92d-596adbe8936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standards_evaluation(evaluation_data, url_page):\n",
    "    \n",
    "    content = get_webpage_text(url_page)\n",
    "\n",
    "    columns_data = str(evaluation_data['metadata']['columns'])\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful program with resources in open data standards and government data publishing practices. When evaluating datasets against standards, provide specific, accurate assessments and maintain strict only JSON and only JSON formatting always.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                content+ \" \" + columns_data + \"\"\"\n",
    "    Given this open data dataset, give me 3 data standards that could be used as a baseline to know which fields to publish an open data set with content that better matches the expectation of the data generated. Be specific of the standards given the theme or the area of the data published, not with generic data standards or open data guidelines, like gtfs for transit, or open contracting for data contracts . Grade this dataset with each of those data standards and provide a json for each of the standards, a structure that follows this one, only respond with a JSON like this:\n",
    "    [\n",
    "      {\n",
    "        \"standard\": \"[standard name]\",\n",
    "        \"match_grade\": \"[grade, green, yellow, red],\n",
    "        \"dataset_link\": \"[standard url]\"\n",
    "      },\n",
    "      {\n",
    "        \"standard\": \"[standard name]\",\n",
    "        \"match_grade\": \"[grade, green, yellow, red]\",\n",
    "        \"dataset_link\": \"[standard url]\"\n",
    "      },\n",
    "      {\n",
    "        \"standard\": \"[standard name]\",\n",
    "        \"match_grade\": \"[grade, green, yellow, red]\",\n",
    "        \"dataset_link\": \"[standard url]\"\n",
    "      }\n",
    "    ]\n",
    "    ONLY AND ONLY RETRIEVE A JSON REPONSE, NOTHING ELSE. No descriptions. Only json\"\"\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "    # chat completion without streaming\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-sonar-large-128k-online\",\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return json.loads(json.loads(response.json())['choices'][0]['message']['content'].replace(\"```\",\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
